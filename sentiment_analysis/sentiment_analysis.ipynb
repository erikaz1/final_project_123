{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing: Sentiment Analysis w/ Cookie Banner Text\n",
    "\n",
    "Recent studies show that pretrained BERT model achieves the highest accuracy on sentiment analysis tasks ([Source](https://typeset.io/questions/what-are-the-best-models-for-sentiment-analysis-2rho2gnvpu#)). BERT accuracy is further enhanced using BiLSTM and BiGRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install torch transformers pandas\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize text, remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer and sentiment analysis pipeline\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_length=508):\n",
    "    '''\n",
    "    Second chunk method: take both 254 words from head and tail: 8 min for 1 batch out of 31    \n",
    "    '''\n",
    "    # Tokenize the entire text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Calculate the number of tokens to select from the beginning and the end\n",
    "    half_max_length = max_length // 2\n",
    "\n",
    "    # Select the first and last portions of the tokens\n",
    "    if len(tokens) > max_length:\n",
    "        beginning_tokens = tokens[:half_max_length]\n",
    "        end_tokens = tokens[-half_max_length:]\n",
    "    else:\n",
    "        beginning_tokens = tokens\n",
    "        end_tokens = []\n",
    "\n",
    "    # Convert tokens back to strings\n",
    "    beginning_chunk = tokenizer.convert_tokens_to_string(beginning_tokens)\n",
    "    end_chunk = tokenizer.convert_tokens_to_string(end_tokens) if end_tokens else ''\n",
    "\n",
    "    # Combine the two chunks into a list, omitting the end chunk if it's empty\n",
    "    chunks = [beginning_chunk, end_chunk] if end_chunk else [beginning_chunk]\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_separate(text_chunks):\n",
    "    '''\n",
    "    Do separate sentiment analysis with chunks\n",
    "    '''\n",
    "    results = sentiment_pipeline(text_chunks)\n",
    "\n",
    "    # Initialize variables to hold weighted sums and total lengths\n",
    "    positive_weighted_sum = 0\n",
    "    negative_weighted_sum = 0\n",
    "    total_length = 0\n",
    "\n",
    "    # Calculate total length for weighting\n",
    "    for chunk in text_chunks:\n",
    "        total_length += len(tokenizer.tokenize(chunk))\n",
    "\n",
    "    # Iterate through results and categorize scores with weights\n",
    "    for chunk, result in zip(text_chunks, results):\n",
    "        chunk_length = len(tokenizer.tokenize(chunk))\n",
    "        if total_length == 0:\n",
    "          # Handle the zero total_length case, e.g., by continuing to the next iteration\n",
    "          weight = 0\n",
    "        else:\n",
    "          weight = chunk_length / total_length  # Calculate weight for the current chunk\n",
    "\n",
    "        if result['label'] == 'POSITIVE':\n",
    "            positive_weighted_sum += result['score'] * weight\n",
    "        else:\n",
    "            # Transforming score to maintain consistency and applying weight\n",
    "            negative_weighted_sum += (1 - result['score']) * weight\n",
    "\n",
    "    # Calculate weighted averages\n",
    "    if total_length > 0:  # Ensure division by zero does not occur\n",
    "        average_positive = positive_weighted_sum\n",
    "        average_negative = negative_weighted_sum\n",
    "    else:\n",
    "        average_positive, average_negative = None, None\n",
    "\n",
    "    return average_positive, average_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lyrics_sentiment_separate(row):\n",
    "    '''\n",
    "    Do sentiment analysis for each row in the record\n",
    "    '''\n",
    "    text_chunks = chunk_text(row['lyrics'])\n",
    "    if not text_chunks:\n",
    "        return None, None\n",
    "    return analyze_sentiment_separate(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_batches(df, start_batch, batch_size=1000):\n",
    "    '''\n",
    "    Do batch processing for the sentiment analysis to avoid data loss due to breakdown\n",
    "    '''\n",
    "    # Number of batches\n",
    "    num_batches = len(df) // batch_size + (1 if len(df) % batch_size else 0)\n",
    "\n",
    "    for i in range(start_batch-1, num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        print(f\"Processing batch {i+1}/{num_batches} (records {start_idx} to {end_idx})\")\n",
    "\n",
    "        # Apply the sentiment analysis function to the batch\n",
    "        batch_results = df.iloc[start_idx:end_idx].apply(lambda row: process_lyrics_sentiment_separate(row), axis=1, result_type='expand')\n",
    "\n",
    "        # Correct assignment to df\n",
    "        df.loc[start_idx:end_idx-1, ['average_positive', 'average_negative']] = batch_results.values # Use .values to assign correctly\n",
    "\n",
    "        # Save the batch results to a CSV file\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        batch_df.to_csv(f'lyrics_sentiment_batch_{i+1}.csv', index=False)\n",
    "\n",
    "        print(f\"Saved batch {i+1} to CSV\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process in batches to avoid data loss - it crashed once, so I changed the start_batch from 0 to 11 to continue\n",
    "# df = process_in_batches(df, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
